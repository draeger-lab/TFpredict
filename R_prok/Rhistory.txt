curr.pred <- lapply(pred.scores, function (x) t(x[,,feat.type]))
class.probs
class.results
class.results[1]
class.results[1][1]
class.results[[1]]
class.results$kStar
class.results$kStar[1]
class.results$kStar[:,1]
class.results$kStar[,1]
class.results$kStar[1,]
length(class.results$kStar)
length(class.results$kStar[1])
class.results$kStar$class.probs
class.results$kStar$class.labels
class.results$kStar[1]
class.results$kStar[[1]
]
class.results$kStar
$class.labels
class.results$kStar$class.labels
names(class.results$kStar)
dimnames(class.results$kStar)
class.results$kStar
help.start()
class(class.results$kStar)
class.results$kStar[[2]]
class.results$kStar[[1]]
length(class.results$kStar[[1]])
class.results$kStar[[1]][[1]]
class.results$kStar[[1]][[2]]
labels
clear
clc
#import.function("read.class.prob.files.R")  OLD VERSION - JTY
source("/home/jtyurkovich/git/TFPredict/R/read.class.prob.files.R")
#import.function("plot.scoring.curve.R")
source("/home/jtyurkovich/git/TFPredict/R/plot.scoring.curve.R")
#import.function("read.inst.names.file.R")
source("/home/jtyurkovich/git/TFPredict/R/read.inst.names.file.R")
#import.function("calc.tpr.R")
source("/home/jtyurkovich/git/TFPredict/R/calc.tpr.R")
#import.function("calc.fpr.R")
source("/home/jtyurkovich/git/TFPredict/R/calc.fpr.R")
# folder containing prob files --> point to this folder - JTY
class.file.dir <- "/home/jtyurkovich/git/TFPredict/resultsFileDir"
# class.file.dir <- "/rahome/eichner/projects/tfpredict/data/super_pred/class_files/latest/"
# read classification outcomes for each feature type and classifier
# this for multiple directories, one for each classifier (put all .prob files in dir) - JTY
# folder for each feature type; keep name allClassifiers
subdirs <- list.dirs(class.file.dir)[-1]
# may not need - JTY
#kstar.idx <- grep("kStar", subdirs)
#if (length(kstar.idx) > 0) {
#  subdirs <- subdirs[-kstar.idx]
#}
class.results <- lapply(subdirs, read.class.prob.files)
feat.names <- sub("_features", "", sub(".*/", "", subdirs))
names(class.results) <- feat.names
method.names  <- names(class.results[[1]])
class.names <- paste("class_", 1:ncol(class.results[[1]][[1]]$class.probs), sep="")
num.classes <- length(class.names)
feat2seq <- read.inst.names.file(subdirs, tf.pred=TRUE)
#feat2seq <- read.inst.names.file(subdirs, tf.pred=FALSE)
names(feat2seq) <- feat.names
seq.names <- names(feat2seq[[1]])
# prepare prediction scores
# TODO: what is contained in pred.scores? there is a data structure for each classifier, but each
#    data structure contains the same header (column names) for each classifier - JTY
if (num.classes == 2) {
pred.scores <- array(NA, dim=c(length(seq.names), length(method.names), length(feat.names)),
dimnames=list(sequence=seq.names, method=method.names, feature=feat.names))
for (i in 1:length(feat.names)) {
pred.scores[,,i] <- sapply(class.results[[i]], function (x) x$class.probs[,1])[feat2seq[[i]],]
}
} else {
pred.scores <- rep(list(array(NA, dim=c(length(seq.names), length(method.names), length(feat.names)),
dimnames=list(sequence=seq.names, method=method.names, feature=feat.names))), num.classes)
for (i in 1:length(feat.names)) {
for (j in 1:length(class.names)) {
pred.scores[[j]][,,i] <- sapply(class.results[[i]], function (x) x$class.probs[,j])[feat2seq[[i]],]
}
}
}
#import.function("read.class.prob.files.R")  OLD VERSION - JTY
source("/home/jtyurkovich/git/TFPredict/R/read.class.prob.files.R")
#import.function("plot.scoring.curve.R")
source("/home/jtyurkovich/git/TFPredict/R/plot.scoring.curve.R")
#import.function("read.inst.names.file.R")
source("/home/jtyurkovich/git/TFPredict/R/read.inst.names.file.R")
#import.function("calc.tpr.R")
source("/home/jtyurkovich/git/TFPredict/R/calc.tpr.R")
#import.function("calc.fpr.R")
source("/home/jtyurkovich/git/TFPredict/R/calc.fpr.R")
# folder containing prob files --> point to this folder - JTY
class.file.dir <- "/home/jtyurkovich/git/TFPredict/resultsFileDir"
# class.file.dir <- "/rahome/eichner/projects/tfpredict/data/super_pred/class_files/latest/"
# read classification outcomes for each feature type and classifier
# this for multiple directories, one for each classifier (put all .prob files in dir) - JTY
# folder for each feature type; keep name allClassifiers
subdirs <- list.dirs(class.file.dir)[-1]
# may not need - JTY
#kstar.idx <- grep("kStar", subdirs)
#if (length(kstar.idx) > 0) {
#  subdirs <- subdirs[-kstar.idx]
#}
class.results <- lapply(subdirs, read.class.prob.files)
feat.names <- sub("_features", "", sub(".*/", "", subdirs))
names(class.results) <- feat.names
method.names  <- names(class.results[[1]])
class.names <- paste("class_", 1:ncol(class.results[[1]][[1]]$class.probs), sep="")
num.classes <- length(class.names)
feat2seq <- read.inst.names.file(subdirs, tf.pred=TRUE)
#feat2seq <- read.inst.names.file(subdirs, tf.pred=FALSE)
names(feat2seq) <- feat.names
seq.names <- names(feat2seq[[1]])
# prepare prediction scores
# TODO: what is contained in pred.scores? there is a data structure for each classifier, but each
#    data structure contains the same header (column names) for each classifier - JTY
if (num.classes == 2) {
pred.scores <- array(NA, dim=c(length(seq.names), length(method.names), length(feat.names)),
dimnames=list(sequence=seq.names, method=method.names, feature=feat.names))
for (i in 1:length(feat.names)) {
pred.scores[,,i] <- sapply(class.results[[i]], function (x) x$class.probs[,1])[feat2seq[[i]],]
}
} else {
pred.scores <- rep(list(array(NA, dim=c(length(seq.names), length(method.names), length(feat.names)),
dimnames=list(sequence=seq.names, method=method.names, feature=feat.names))), num.classes)
for (i in 1:length(feat.names)) {
for (j in 1:length(class.names)) {
pred.scores[[j]][,,i] <- sapply(class.results[[i]], function (x) x$class.probs[,j])[feat2seq[[i]],]
}
}
}
# prepare labels
global.labels <- class.results$pssm[[1]]$class.labels[feat2seq$pssm]
if (num.classes == 2) {
labels <- global.labels
labels[labels == 2] <- -1
pred.task <- "TF/non-TF"
} else {
labels <- list()
for (i in 1:num.classes) {
curr.labels <- global.labels
curr.labels[curr.labels != i] <- -1
curr.labels[curr.labels == i] <- 1
labels[[i]] <-  curr.labels
pred.task <- "superclass"
}
}
# remove sequences for which no prediction was possible due to missing domains
if (num.classes == 2) {
no.pred.seq <- union(which(is.na(pred.scores[,1,1])), which(is.na(pred.scores[,1,3])))
if (length(no.pred.seq) > 0) {
pred.scores <- pred.scores[-no.pred.seq,,]
labels <- labels[-no.pred.seq]
}
} else {
no.pred.seq <- which(is.na((pred.scores[[1]][,1,1])))
if (length(no.pred.seq) > 0) {
pred.scores <- lapply(pred.scores, function (x) x[-no.pred.seq,,])
labels <- lapply(labels, function (x) x[-no.pred.seq])
}
}
labels
gloabl.labels
length(no.pred.seq)
dim(pred.scores)
labels
global.labels
names[class.results]
names(class.results)
names(feat2seq)
global.labels <- class.results$pssm[[1]]$class.labels[feat2seq$allClassifiers]
global.labels <- class.results$allClassifiers[[1]]$class.labels[feat2seq$allClassifiers]
global.labels
# prepare labels
global.labels <- class.results$pssm[[1]]$class.labels[feat2seq$pssm]
if (num.classes == 2) {
labels <- global.labels
labels[labels == 2] <- -1
pred.task <- "TF/non-TF"
} else {
labels <- list()
for (i in 1:num.classes) {
curr.labels <- global.labels
curr.labels[curr.labels != i] <- -1
curr.labels[curr.labels == i] <- 1
labels[[i]] <-  curr.labels
pred.task <- "superclass"
}
}
# remove sequences for which no prediction was possible due to missing domains
if (num.classes == 2) {
no.pred.seq <- union(which(is.na(pred.scores[,1,1])), which(is.na(pred.scores[,1,3])))
if (length(no.pred.seq) > 0) {
pred.scores <- pred.scores[-no.pred.seq,,]
labels <- labels[-no.pred.seq]
}
} else {
no.pred.seq <- which(is.na((pred.scores[[1]][,1,1])))
if (length(no.pred.seq) > 0) {
pred.scores <- lapply(pred.scores, function (x) x[-no.pred.seq,,])
labels <- lapply(labels, function (x) x[-no.pred.seq])
}
}
# prepare labels
#global.labels <- class.results$pssm[[1]]$class.labels[feat2seq$pssm]
global.labels <- class.results$allClassifiers[[1]]$class.labels[feat2seq$allClassifiers]
if (num.classes == 2) {
labels <- global.labels
labels[labels == 2] <- -1
pred.task <- "TF/non-TF"
} else {
labels <- list()
for (i in 1:num.classes) {
curr.labels <- global.labels
curr.labels[curr.labels != i] <- -1
curr.labels[curr.labels == i] <- 1
labels[[i]] <-  curr.labels
pred.task <- "superclass"
}
}
# remove sequences for which no prediction was possible due to missing domains
if (num.classes == 2) {
no.pred.seq <- union(which(is.na(pred.scores[,1,1])), which(is.na(pred.scores[,1,3])))
if (length(no.pred.seq) > 0) {
pred.scores <- pred.scores[-no.pred.seq,,]
labels <- labels[-no.pred.seq]
}
} else {
no.pred.seq <- which(is.na((pred.scores[[1]][,1,1])))
if (length(no.pred.seq) > 0) {
pred.scores <- lapply(pred.scores, function (x) x[-no.pred.seq,,])
labels <- lapply(labels, function (x) x[-no.pred.seq])
}
}
global.labels
labels
sapply(labels, table)
sapply(labels, length)
table(labels)
# ROC curves and bar plots for comparison of classifiers (single feature type)
roc.outfile <- paste(class.file.dir, "ROC_curves.pdf", sep="")
roc.scores <- list()
pdf(roc.outfile, width=12, height=8)
for (feat.type in feat.names) {
if (num.classes == 2) {
curr.pred <- t(pred.scores[,,feat.type])
curr.lab <- labels+2
} else {
curr.pred <- lapply(pred.scores, function (x) t(x[,,feat.type]))
}
roc.scores[[feat.type]] <- plot.scoring.curve(curr.pred, labels, outfile=NULL, auc.barplot=TRUE, use.layout=TRUE, add.roc.point=c(mean.fpr, mean.tpr),
title=sprintf("ROC curve for %s predictors using %s features", pred.task, feat.type))
}
dev.off()
options(debug=recover)
options(error=recover)
# ROC curves and bar plots for comparison of classifiers (single feature type)
roc.outfile <- paste(class.file.dir, "ROC_curves.pdf", sep="")
roc.scores <- list()
pdf(roc.outfile, width=12, height=8)
for (feat.type in feat.names) {
if (num.classes == 2) {
curr.pred <- t(pred.scores[,,feat.type])
curr.lab <- labels+2
} else {
curr.pred <- lapply(pred.scores, function (x) t(x[,,feat.type]))
}
roc.scores[[feat.type]] <- plot.scoring.curve(curr.pred, labels, outfile=NULL, auc.barplot=TRUE, use.layout=TRUE, add.roc.point=c(mean.fpr, mean.tpr),
title=sprintf("ROC curve for %s predictors using %s features", pred.task, feat.type))
}
ls
ls()
# ROC curves and bar plots for comparison of classifiers (single feature type)
roc.outfile <- paste(class.file.dir, "ROC_curves.pdf", sep="")
roc.scores <- list()
pdf(roc.outfile, width=12, height=8)
for (feat.type in feat.names) {
if (num.classes == 2) {
curr.pred <- t(pred.scores[,,feat.type])
curr.lab <- labels+2
} else {
curr.pred <- lapply(pred.scores, function (x) t(x[,,feat.type]))
}
roc.scores[[feat.type]] <- plot.scoring.curve(curr.pred, labels, outfile=NULL, auc.barplot=TRUE, use.layout=TRUE, #add.roc.point=c(mean.fpr, mean.tpr),
title=sprintf("ROC curve for %s predictors using %s features", pred.task, feat.type))
}
dev.off()
# ROC curves and bar plots for comparison of classifiers (single feature type)
roc.outfile <- paste(class.file.dir, "ROC_curves.pdf", sep="")
roc.scores <- list()
pdf(roc.outfile, width=12, height=8)
for (feat.type in feat.names) {
if (num.classes == 2) {
curr.pred <- t(pred.scores[,,feat.type])
curr.lab <- labels+2
} else {
curr.pred <- lapply(pred.scores, function (x) t(x[,,feat.type]))
}
roc.scores[[feat.type]] <- plot.scoring.curve(curr.pred, labels, outfile=NULL, auc.barplot=TRUE, use.layout=TRUE, #add.roc.point=c(mean.fpr, mean.tpr),
title=sprintf("ROC curve for %s predictors using %s features", pred.task, feat.type))
}
dev.off()
pred.scores
dim(pred.scores)
dimnames(pred.scores)[2:3]
pred.scores <- pred.scores[,,1,drop=F]
dimnames(pred.scores)
dimnames(pred.scores)[2:3]
# ROC curves and bar plots for comparison of classifiers (single feature type)
roc.outfile <- paste(class.file.dir, "ROC_curves.pdf", sep="")
roc.scores <- list()
pdf(roc.outfile, width=12, height=8)
for (feat.type in feat.names) {
if (num.classes == 2) {
curr.pred <- t(pred.scores[,,feat.type])
curr.lab <- labels+2
} else {
curr.pred <- lapply(pred.scores, function (x) t(x[,,feat.type]))
}
roc.scores[[feat.type]] <- plot.scoring.curve(curr.pred, labels, outfile=NULL, auc.barplot=TRUE, use.layout=TRUE, #add.roc.point=c(mean.fpr, mean.tpr),
title=sprintf("ROC curve for %s predictors using %s features", pred.task, feat.type))
}
apply(pred.scores, 2, summary)
con.pred.scores <- apply(pred.scores[,,1], 1, mean)
con.pred.scores
length(con.pred.scores)
length(labels)
pred.scores[,8,1] <- con.pred.scores
dim(pred.scores[,8,1])
dim(pred.scores)
pred.scores <- cbind(pred.scores[,,1], con.pred.scores)
dim(pred.scores)
dim(pred.scores) <- c(dim(pred.scores), 1)
dim(pred.scores)
# ROC curves and bar plots for comparison of classifiers (single feature type)
roc.outfile <- paste(class.file.dir, "ROC_curves.pdf", sep="")
roc.scores <- list()
pdf(roc.outfile, width=12, height=8)
for (feat.type in feat.names) {
if (num.classes == 2) {
curr.pred <- t(pred.scores[,,feat.type])
curr.lab <- labels+2
} else {
curr.pred <- lapply(pred.scores, function (x) t(x[,,feat.type]))
}
roc.scores[[feat.type]] <- plot.scoring.curve(curr.pred, labels, outfile=NULL, auc.barplot=TRUE, use.layout=TRUE, #add.roc.point=c(mean.fpr, mean.tpr),
title=sprintf("ROC curve for %s predictors using %s features", pred.task, feat.type))
}
feat.type
dimnames
dimnames(pred.scores)
dimnames.james=list(sequence=seq.names, method=method.names, feature=feat.names)
dimnames.james
dim(dimnames.james)
sapply(dimnames.james, length)
dim(dimnames.james)
dimnames.james=list(sequence=seq.names, method=c("allClassifiers", "consensus"), feature="percentile")
sapply(dimnames.james, length)
dim(pred.scores)
pred.scores <- pred.scores[,c(1,8),,drop=F]
dim(pred.scores)
dimnames(pred.scores)
dimnames(pred.scores) <- dimnames.james
# ROC curves and bar plots for comparison of classifiers (single feature type)
roc.outfile <- paste(class.file.dir, "ROC_curves.pdf", sep="")
roc.scores <- list()
pdf(roc.outfile, width=12, height=8)
for (feat.type in feat.names) {
if (num.classes == 2) {
curr.pred <- t(pred.scores[,,feat.type])
curr.lab <- labels+2
} else {
curr.pred <- lapply(pred.scores, function (x) t(x[,,feat.type]))
}
roc.scores[[feat.type]] <- plot.scoring.curve(curr.pred, labels, outfile=NULL, auc.barplot=TRUE, use.layout=TRUE, #add.roc.point=c(mean.fpr, mean.tpr),
title=sprintf("ROC curve for %s predictors using %s features", pred.task, feat.type))
}
feat.names
feat.names <- c("allClassifiers", "consensus")
# ROC curves and bar plots for comparison of classifiers (single feature type)
roc.outfile <- paste(class.file.dir, "ROC_curves.pdf", sep="")
roc.scores <- list()
pdf(roc.outfile, width=12, height=8)
for (feat.type in feat.names) {
if (num.classes == 2) {
curr.pred <- t(pred.scores[,,feat.type])
curr.lab <- labels+2
} else {
curr.pred <- lapply(pred.scores, function (x) t(x[,,feat.type]))
}
roc.scores[[feat.type]] <- plot.scoring.curve(curr.pred, labels, outfile=NULL, auc.barplot=TRUE, use.layout=TRUE, #add.roc.point=c(mean.fpr, mean.tpr),
title=sprintf("ROC curve for %s predictors using %s features", pred.task, feat.type))
}
feat.type
dimnames(pred.scores[3])
dimnames(pred.scores)[3]
feat.names <- "percentile"
# ROC curves and bar plots for comparison of classifiers (single feature type)
roc.outfile <- paste(class.file.dir, "ROC_curves.pdf", sep="")
roc.scores <- list()
pdf(roc.outfile, width=12, height=8)
for (feat.type in feat.names) {
if (num.classes == 2) {
curr.pred <- t(pred.scores[,,feat.type])
curr.lab <- labels+2
} else {
curr.pred <- lapply(pred.scores, function (x) t(x[,,feat.type]))
}
roc.scores[[feat.type]] <- plot.scoring.curve(curr.pred, labels, outfile=NULL, auc.barplot=TRUE, use.layout=TRUE, #add.roc.point=c(mean.fpr, mean.tpr),
title=sprintf("ROC curve for %s predictors using %s features", pred.task, feat.type))
}
dev.off()
save.image("~/git/TFPredict/R/johannes_help_12_16.RData")
#import.function("read.class.prob.files.R")  OLD VERSION - JTY
source("/home/jtyurkovich/git/TFPredict/R/read.class.prob.files.R")
#import.function("plot.scoring.curve.R")
source("/home/jtyurkovich/git/TFPredict/R/plot.scoring.curve.R")
#import.function("read.inst.names.file.R")
source("/home/jtyurkovich/git/TFPredict/R/read.inst.names.file.R")
#import.function("calc.tpr.R")
source("/home/jtyurkovich/git/TFPredict/R/calc.tpr.R")
#import.function("calc.fpr.R")
source("/home/jtyurkovich/git/TFPredict/R/calc.fpr.R")
# folder containing prob files --> point to this folder - JTY
class.file.dir <- "/home/jtyurkovich/git/TFPredict/resultsFileDir"
# class.file.dir <- "/rahome/eichner/projects/tfpredict/data/super_pred/class_files/latest/"
# read classification outcomes for each feature type and classifier
# this for multiple directories, one for each classifier (put all .prob files in dir) - JTY
# folder for each feature type; keep name allClassifiers
subdirs <- list.dirs(class.file.dir)[-1]
# may not need - JTY
#kstar.idx <- grep("kStar", subdirs)
#if (length(kstar.idx) > 0) {
#  subdirs <- subdirs[-kstar.idx]
#}
class.results <- lapply(subdirs, read.class.prob.files)
feat.names <- sub("_features", "", sub(".*/", "", subdirs))
names(class.results) <- feat.names
method.names  <- names(class.results[[1]])
class.names <- paste("class_", 1:ncol(class.results[[1]][[1]]$class.probs), sep="")
num.classes <- length(class.names)
feat2seq <- read.inst.names.file(subdirs, tf.pred=TRUE)
#feat2seq <- read.inst.names.file(subdirs, tf.pred=FALSE)
names(feat2seq) <- feat.names
seq.names <- names(feat2seq[[1]])
# prepare prediction scores
# TODO: what is contained in pred.scores? there is a data structure for each classifier, but each
#    data structure contains the same header (column names) for each classifier - JTY
if (num.classes == 2) {
pred.scores <- array(NA, dim=c(length(seq.names), length(method.names), length(feat.names)),
dimnames=list(sequence=seq.names, method=method.names, feature=feat.names))
for (i in 1:length(feat.names)) {
pred.scores[,,i] <- sapply(class.results[[i]], function (x) x$class.probs[,1])[feat2seq[[i]],]
}
} else {
pred.scores <- rep(list(array(NA, dim=c(length(seq.names), length(method.names), length(feat.names)),
dimnames=list(sequence=seq.names, method=method.names, feature=feat.names))), num.classes)
for (i in 1:length(feat.names)) {
for (j in 1:length(class.names)) {
pred.scores[[j]][,,i] <- sapply(class.results[[i]], function (x) x$class.probs[,j])[feat2seq[[i]],]
}
}
}
# prepare labels
#global.labels <- class.results$pssm[[1]]$class.labels[feat2seq$pssm]
global.labels <- class.results$allClassifiers[[1]]$class.labels[feat2seq$allClassifiers]
if (num.classes == 2) {
labels <- global.labels
labels[labels == 2] <- -1
pred.task <- "TF/non-TF"
} else {
labels <- list()
for (i in 1:num.classes) {
curr.labels <- global.labels
curr.labels[curr.labels != i] <- -1
curr.labels[curr.labels == i] <- 1
labels[[i]] <-  curr.labels
pred.task <- "superclass"
}
}
# remove sequences for which no prediction was possible due to missing domains
if (num.classes == 2) {
no.pred.seq <- union(which(is.na(pred.scores[,1,1])), which(is.na(pred.scores[,1,3])))
if (length(no.pred.seq) > 0) {
pred.scores <- pred.scores[-no.pred.seq,,]
labels <- labels[-no.pred.seq]
}
} else {
no.pred.seq <- which(is.na((pred.scores[[1]][,1,1])))
if (length(no.pred.seq) > 0) {
pred.scores <- lapply(pred.scores, function (x) x[-no.pred.seq,,])
labels <- lapply(labels, function (x) x[-no.pred.seq])
}
}
# ROC curves and bar plots for comparison of classifiers (single feature type)
roc.outfile <- paste(class.file.dir, "ROC_curves.pdf", sep="")
roc.scores <- list()
pdf(roc.outfile, width=12, height=8)
for (feat.type in feat.names) {
if (num.classes == 2) {
curr.pred <- t(pred.scores[,,feat.type])
curr.lab <- labels+2
} else {
curr.pred <- lapply(pred.scores, function (x) t(x[,,feat.type]))
}
roc.scores[[feat.type]] <- plot.scoring.curve(curr.pred, labels, outfile=NULL, auc.barplot=TRUE, use.layout=TRUE, #add.roc.point=c(mean.fpr, mean.tpr),
title=sprintf("ROC curve for %s predictors using %s features", pred.task, feat.type))
}
dev.off()
